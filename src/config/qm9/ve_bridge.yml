model:
  denoiser:
    bridge_type: 've'
    beta_d: 2
    beta_min: 0.1
    sigma_data: 0.5
    sigma_max: 80
    sigma_min: 0.002
    schedule_sampler: 'real-uniform'
    use_ema: true
    ema_decay: 0.999
    weight_decay: 1e-12
    weight_schedule: 'uniform' # 'uniform'
    loss_x_weight: 1e+1

  backbone: 
    type: 'SE3Transformer'
    feature_size: 8
    num_blocks: 1
    num_layers: 10
    hidden_size: 128
    num_heads: 16
    edge_feat_dim: 4  # edge type feat
    num_r_gaussian: 20
    knn: 16 # !
    num_node_types: 8
    act_fn: relu
    norm: True
    cutoff_mode: knn  # [radius, none]
    ew_net_type: global  # [r, m, none]
    num_x2h: 1
    num_h2x: 1
    r_max: 10.
    x2h_out_fc: False
    sync_twoup: False
    time_cond: True
    xT_type: 'noise' # 'noise' # 'none' # 'pp'
    xT_mode: 'concat_graph' # 'concat_graph' # 'none'

training:
  batch_size: 128
  log_interval: 100
  save_interval: 1000
  test_interval: 1000
  total_training_steps: 10000000
  learning_rate: 0.01
  lr_anneal_steps: 0
  max_epochs: 100
  use_lr_scheduler: true
  lr_scheduler_config: # 10000 warmup steps
    target: model.utils.lr_scheduler.LambdaLinearScheduler
    params:
      warm_up_steps: [10000]
      cycle_lengths: [10000000000000]
      f_start: [1.e-6]
      f_max: [1.]
      f_min: [ 1.]

sampling:
  batch_size: 1000

data:
  root: ../data/qm9
  max_node_num: 29
  coord_dim: 3
  module: QM9Dataset # CombinedGraphDataset # PharmacophoreDataset